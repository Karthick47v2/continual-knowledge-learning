{
    "finetuning_ratio": 0.1,
    "input_length" : 50,
    "output_length" : 50,
    "num_train_epochs" : 5,
    "output_dir" : "",
    "dataset" : "recentqa",
    "dataset_version" : "small",
    "train_batch_size" : 8,
    "learning_rate" : 1e-3,
    "model" : "gpt2-large",
    "method": "lora",
    "freeze_level": 0,
    "gradient_accumulation_steps" : 1,
    "ngpu" : 4,
    "num_workers" : 0,
    "resume_from_checkpoint" : null,
    "accelerator" : "ddp",
    "use_deepspeed" : false,
    "CUDA_VISIBLE_DEVICES" : "0,1,2,3",
    "wandb_log": true,
    "wandb_project": "continual_learning_gpt2",
    "wandb_run_name" : "GPT2_large_lora_recentqa_finetuning",
    "mode" : "finetune",
    "use_lr_scheduling" : false,
    "check_validation" : false,
    "checkpoint_path" : "outputs/gpt2/gpt2/lora.ckpt"
}