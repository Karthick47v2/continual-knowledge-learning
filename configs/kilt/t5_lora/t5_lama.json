{
    "finetuning_ratio": 0.05,
    "input_length" : 25,
    "output_length" : 4,
    "num_train_epochs" : 1,
    "output_dir" : "",
    "dataset" : "lama",
    "dataset_version" : "small",
    "train_batch_size" : 32,
    "learning_rate" : 1e-3,
    "model" : "google/t5-large-ssm",
    "method": "lora",
    "freeze_level": 0,
    "gradient_accumulation_steps" : 1,
    "ngpu" : 4,
    "num_workers" : 40,
    "resume_from_checkpoint" : null,
    "accelerator" : "ddp",
    "use_deepspeed" : false,
    "CUDA_VISIBLE_DEVICES" : "0,1,2,3",
    "wandb_log": true,
    "wandb_project": "continual_learning_finetuning",
    "wandb_run_name" : "T5_large_lora_lama_finetuning_baseline",
    "mode" : "finetune",
    "use_lr_scheduling" : false,
    "check_validation" : false,
    "checkpoint_path" : "output/last_lora.ckpt"
}